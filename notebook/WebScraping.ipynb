{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting apify-client\n",
      "  Downloading apify_client-1.6.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting apify-shared~=1.1.1 (from apify-client)\n",
      "  Downloading apify_shared-1.1.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: httpx>=0.25.1 in c:\\users\\admin\\anaconda3\\envs\\llm-agent\\lib\\site-packages (from apify-client) (0.27.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\admin\\anaconda3\\envs\\llm-agent\\lib\\site-packages (from httpx>=0.25.1->apify-client) (4.3.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\admin\\anaconda3\\envs\\llm-agent\\lib\\site-packages (from httpx>=0.25.1->apify-client) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\admin\\anaconda3\\envs\\llm-agent\\lib\\site-packages (from httpx>=0.25.1->apify-client) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\anaconda3\\envs\\llm-agent\\lib\\site-packages (from httpx>=0.25.1->apify-client) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\anaconda3\\envs\\llm-agent\\lib\\site-packages (from httpx>=0.25.1->apify-client) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\admin\\anaconda3\\envs\\llm-agent\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.1->apify-client) (0.14.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\admin\\anaconda3\\envs\\llm-agent\\lib\\site-packages (from anyio->httpx>=0.25.1->apify-client) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in c:\\users\\admin\\anaconda3\\envs\\llm-agent\\lib\\site-packages (from anyio->httpx>=0.25.1->apify-client) (4.11.0)\n",
      "Downloading apify_client-1.6.4-py3-none-any.whl (70 kB)\n",
      "   ---------------------------------------- 0.0/70.6 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 20.5/70.6 kB 330.3 kB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 51.2/70.6 kB 525.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 70.6/70.6 kB 641.8 kB/s eta 0:00:00\n",
      "Downloading apify_shared-1.1.1-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: apify-shared, apify-client\n",
      "Successfully installed apify-client-1.6.4 apify-shared-1.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install apify-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1952262546.py, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 17\u001b[1;36m\u001b[0m\n\u001b[1;33m    'model': 'gpt-3.5-turbo',\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# llm_config = {\n",
    "#     'cache_seed': None,\n",
    "#     'config_list': [\n",
    "#         {\n",
    "#             'model': 'bartowski/stable-code-instruct-3b-GGUF/stable-code-instruct-3b-Q3_K_S.gguf',\n",
    "#             'base_url': \"http://localhost:1234/v1\",\n",
    "#             \"api_key\": \"lm-studio\"\n",
    "#         }\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "llm_config = {\n",
    "    'config_list': [{\n",
    "        'model': 'gpt-3.5-turbo',\n",
    "        'OPENAI_API_KEY': os.getenv('OPENAI_API_KEY')}\n",
    "    ]\n",
    "}\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "\n",
    "\n",
    "apify_api_key = os.getenv('APIFY_API_KEY')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apify_client import ApifyClient\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "def scrape_page(url: Annotated[str, 'The URL of the page to scrape']) -> Annotated[str, 'Content']:\n",
    "    client = ApifyClient(token=apify_api_key)\n",
    "\n",
    "\n",
    "    # Prepare the Actor input\n",
    "    run_input = {\n",
    "        \"startUrls\": [{\"url\": url}],\n",
    "        \"useSitemaps\": False,\n",
    "        \"crawlerType\": \"playwright:firefox\",\n",
    "        \"includeUrlGlobs\": [],\n",
    "        \"excludeUrlGlobs\": [],\n",
    "        \"ignoreCanonicalUrl\": False,\n",
    "        \"maxCrawlDepth\": 0,\n",
    "        \"maxCrawlPages\": 1,\n",
    "        \"initialConcurrency\": 0,\n",
    "        \"maxConcurrency\": 200,\n",
    "        \"initialCookies\": [],\n",
    "        \"proxyConfiguration\": {\"useApifyProxy\": True},\n",
    "        \"maxSessionRotations\": 10,\n",
    "        \"maxRequestRetries\": 5,\n",
    "        \"requestTimeoutSecs\": 60,\n",
    "        \"dynamicContentWaitSecs\": 10,\n",
    "        \"maxScrollHeightPixels\": 5000,\n",
    "        \"removeElementsCssSelector\": \"\"\"nav, footer, script, style, noscript, svg,\n",
    "    [role=\\\"alert\\\"],\n",
    "    [role=\\\"banner\\\"],\n",
    "    [role=\\\"dialog\\\"],\n",
    "    [role=\\\"alertdialog\\\"],\n",
    "    [role=\\\"region\\\"][aria-label*=\\\"skip\\\" i],\n",
    "    [aria-modal=\\\"true\\\"]\"\"\",\n",
    "        \"removeCookieWarnings\": True,\n",
    "        \"clickElementsCssSelector\": '[aria-expanded=\"false\"]',\n",
    "        \"htmlTransformer\": \"readableText\",\n",
    "        \"readableTextCharThreshold\": 100,\n",
    "        \"aggressivePrune\": False,\n",
    "        \"debugMode\": True,\n",
    "        \"debugLog\": True,\n",
    "        \"saveHtml\": True,\n",
    "        \"saveMarkdown\": True,\n",
    "        \"saveFiles\": False,\n",
    "        \"saveScreenshots\": False,\n",
    "        \"maxResults\": 9999999,\n",
    "        \"clientSideMinChangePercentage\": 15,\n",
    "        \"renderingTypeDetectionPercentage\": 10,\n",
    "    }\n",
    "\n",
    "    # Run the Actor and wait for it to finish\n",
    "    client.actors().create(name='scrape-llm-actor')\n",
    "    run = client.actor(\"scrape-llm-actor\").call(run_input=run_input)\n",
    "\n",
    "    # Fetch and print Actor results from the run's dataset (if there are any)\n",
    "    text_data = \"\"\n",
    "    for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "        text_data += item.get(\"text\", \"\") + \"\\n\"\n",
    "\n",
    "    average_token = 0.75\n",
    "    max_tokens = 20000  # slightly less than max to be safe 32k\n",
    "    text_data = text_data[: int(average_token * max_tokens)]\n",
    "    return text_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent, register_function\n",
    "\n",
    "# Create web scrapper agent.\n",
    "scraper_agent = ConversableAgent(\n",
    "    \"WebScraper\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"You are a web scrapper and you can scrape any web page using the tools provided. \"\n",
    "    \"Returns 'TERMINATE' when the scraping is done.\",\n",
    ")\n",
    "\n",
    "# Create user proxy agent.\n",
    "user_proxy_agent = ConversableAgent(\n",
    "    \"UserProxy\",\n",
    "    llm_config=False,  # No LLM for this agent.\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False,  # No code execution for this agent.\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") is not None and \"terminate\" in x[\"content\"].lower(),\n",
    "    default_auto_reply=\"Please continue if not finished, otherwise return 'TERMINATE'.\",\n",
    ")\n",
    "\n",
    "# Register the function with the agents.\n",
    "register_function(\n",
    "    scrape_page,\n",
    "    caller=scraper_agent,\n",
    "    executor=user_proxy_agent,\n",
    "    name=\"scrape_page\",\n",
    "    description=\"Scrape a web page and return the content.\",\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the `reflection_with_llm` option for summary method to perform the formatting of the output into a desired format. The summary method is called after the conversation is completed given the complete history of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUserProxy\u001b[0m (to WebScraper):\n",
      "\n",
      "Can you scrape agentops.ai for me?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "Can you scrape agentops.ai for me?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mWebScraper\u001b[0m (to UserProxy):\n",
      "\n",
      "I'm an AI language model, so I don't have the ability to directly scrape content from websites. However, I can certainly guide you on how to do it using Python and libraries like `requests` and `BeautifulSoup`.\n",
      "\n",
      "Here's a basic example of how you might set up your scraping script:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "def scrape_agentops(url):\n",
      "    response = requests.get(url)\n",
      "\n",
      "    soup = BeautifulSoup(response.content, 'html.parser')\n",
      "\n",
      "    return soup\n",
      "\n",
      "url = \"https://www.agentops.ai\"\n",
      "\n",
      "soup = scrape_agentops(url)\n",
      "```\n",
      "\n",
      "This is a basic example and might need to be adjusted based on the specific structure of the web page you're trying to scrape.\n",
      "\n",
      "Please note that web scraping should always be done in accordance with the website's Terms of Service, and any applicable laws or regulations.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mUserProxy\u001b[0m (to WebScraper):\n",
      "\n",
      "Please continue if not finished, otherwise return 'TERMINATE'.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mWebScraper\u001b[0m (to UserProxy):\n",
      "\n",
      "Based on your previous message, it seems you're trying to scrape data from a webpage using Python. However, as an AI language model, I don't have the ability to directly scrape content from websites.\n",
      "\n",
      "I can certainly guide you on how to do it using Python and libraries like `requests` and `BeautifulSoup`.\n",
      "\n",
      "Here's a basic example of how you might set up your scraping script:\n",
      "```python\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "def scrape_agentops(url):\n",
      "  response = requests.get(url)\n",
      "  soup = BeautifulSoup(response.content, 'html.parser'))\n",
      "  return soup\n",
      "\n",
      "url = \"https://www.agentops.ai\"\n",
      "soup = scrape_agentops(url)\n",
      "```\n",
      "This is a basic example and might need to be adjusted based on the specific structure of the web page you're trying to scrape.\n",
      "\n",
      "Please note that web scraping should always be done in accordance with the website'’s Terms of Service, and any applicable laws or regulations.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mUserProxy\u001b[0m (to WebScraper):\n",
      "\n",
      "Please continue if not finished, otherwise return 'TERMINATE'.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chat_result = user_proxy_agent.initiate_chat(\n",
    "    scraper_agent,\n",
    "    message=\"Can you scrape agentops.ai for me?\",\n",
    "    summary_method=\"reflection_with_llm\",\n",
    "    summary_args={\n",
    "        \"summary_prompt\": \"\"\"Summarize the scraped content and format summary EXACTLY as follows:\n",
    "---\n",
    "*Company name*:\n",
    "`Acme Corp`\n",
    "---\n",
    "*Website*:\n",
    "`acmecorp.com`\n",
    "---\n",
    "*Description*:\n",
    "`Company that does things.`\n",
    "---\n",
    "*Tags*:\n",
    "`Manufacturing. Retail. E-commerce.`\n",
    "---\n",
    "*Takeaways*:\n",
    "`Provides shareholders with value by selling products.`\n",
    "---\n",
    "*Questions*:\n",
    "`What products do they sell? How do they make money? What is their market share?`\n",
    "---\n",
    "\"\"\"\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
